---
title: "DATA 605: Final Project"
subtitle: "by Dmitriy Vecheruk"
date: "20 May 2017"
output:
  html_document:
    theme: yeti
    highlight: kate
    toc: true
    toc_float: true
    code_folding: show
    
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               fig.height = 4)
opts_knit$set(width=75)

```

# 1. Introduction

This document is my submission for the final project assignment of the CUNY MSDA DATA 605 course (Spring 2017).  

The goal of the assignment is to use a subset of variables from the [Kaggle Housing Price dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) and apply various data analysis techniques from the following domains on it:
  
1) Probability calculation and independence test  
2) Descriptive and inferential statistics  
3) Linear algebra and correlation  
4) Calculus-based probability and statistics  
5) Regression modeling
  
The details of the assignments are provided below.


# 2. Data preparation
  
### Dataset inspection and subsetting

First off, we are required to read the training data and pick a quantitative variable that should be the independent variable X, whereas the dependent variable Y is `SalePrice`.
  
```{r message=F, warning=F, cache=T}
library(MASS)
library(dplyr)
library(ggplot2)

inp = read_csv("https://raw.githubusercontent.com/datafeelings/data_605_final_project/master/data/train.csv")
inp = data.frame(inp)
str(inp)
```
  
This training dataset at hand is a subset of rows from the full Ames Housing Dataset.  
It contains `r nrow(inp)` rows and `r ncol(inp)` columns, many of which are categorical.   
The dataset describes various properties of houses that were sold in Ames, Iowa, and their sale price.

After inspecting the codebook^[1][1]^, the following quantitative columns were selected as potentially interesting independent variables:
`LotArea` (sq.ft. size of the lot) and `GrLivArea`(sq.ft. size of the living area above ground).

We'll first inspect the filling rates and distributions of each of the variables and then pick one to proceed with the assignment.

```{r}

# Exclude the Id variable from all further analysis
inp = inp[,2:ncol(inp)]

inp = as.tbl(inp)
inp1 = inp %>% dplyr::select(LotArea,GrLivArea,SalePrice)
summary(inp1)
```

We can see that none of the variables has missing values, so we can proceed to the distribution plots^[2][2]^:

```{r}
library(ggplot2)

inp1long <- reshape2::melt(inp1)
ggplot(inp1long, aes(value)) + facet_wrap(~variable, scales = 'free_x') +
  geom_histogram()
```

From the plots we see that `GrLivArea` and `SalePrice` are approximately normal (with some skew and of course no negative values), and `LotArea` has some very frequent values, and also a few very extreme outliers.
So we will use `GrLivArea` as the independent variable (X), and `SalePrice` as the dependent variable(Y) in the subsequent analysis.
  
```{r}
dataset = inp1 %>% dplyr::select(X = GrLivArea, Y = SalePrice)
kable(head(dataset))
```

  
# 3. Solution
## 3.1 Probability calculation and independence test 
    
Assignment: *Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the 4th quartile of the X variable, and the small letter "y" is estimated as the 2d quartile of the Y variable.*  
*Interpret the meaning of all probabilities.*  
  
a. P(X > x | Y > y)	 	
b. P(X > x, Y > y)	 	
c. P(X < x | Y > y)  		

*Does splitting the training data in this fashion make them independent? In other words, does P(X|Y)=P(X)P(Y))? Check mathematically, and then evaluate by running a Chi Square test for association. *

For the purporses of this assignment, we shall interpret 4th quartile and 2nd quartile as the **lower quartile bounds**: 2nd quartile = 25th percentile, 4th quartile = 47th percentile.

Then we split the variables according to the specification and calculate the probabilities by counting the number of rows in each group:

```{r}
dataset = dataset %>% 
  mutate(X_over_P75 = X > quantile(X,0.75),
         Y_over_P25 = Y > quantile(Y, 0.25))

table(dplyr::select(dataset,X_over_P75, Y_over_P25))

print (paste("Total number of rows:",nrow(dataset)))
```

From the contingency table above we can calculate:
  
a. P(X > x | Y > y) - this can be interpreted as the probability of observing high values of X (in the top 25%) in the subset of the top 75% of the values of Y.
  
P(X > x | Y > y)= P (X > x , Y > y) / P (Y > y) = (351/1460)/ 0.75 = `r (351/1460)/ 0.75`
  
So for the values of Y (house selling price) that are not low, in 32% of cases the values of X (living area of the house) will be high. 

b. P(X > x , Y > y) - this can be interpreted as the probability of observing high values of X (in the top 25%) and the values of Y in the top 75%:
P (X > x , Y > y) = 351/1460 = `r (351/1460)`
  
So 24% of all houses that were sold at a higher price also had not the smallest living area.
  
c. P(X < x | Y > y) = 1 - P(X > x | Y > y) = 1 - 0.3205479 = `r 1 - 0.3205479`

This is a complement probability to (a) and can be interpreted as follows: For the houses with not the lowest living area, the probability of selling at a high price is 68%.
  
Now we check if splitting the data in such a way make it independent, i.e. P(X|Y)=P(X) 
or P(X,Y) = P(X)P(Y).
  
P(X > x | Y > y) = 0.32, but P(X > x) = 0.25, so the condition is not fullfilled  
P (X > x , Y > y) = 0.24, but P(X > x)P(Y > y) = 0.25 * 0.75 = 0.1875, so the condition is not fullfilled

Thus, splitting the data is such a way does not make them independent.

Now we can test the independence of the distributions with a Chi Square test^[3][3]^:

```{r}
chisq.test(as.numeric(dataset$X_over_P75),as.numeric(dataset$Y_over_P25))
```

As the p-value of the test statistic is very small, we reject H~0~ that the distribution of the values between the groups is independent in favor of H~A~ that the distribution of the observations in the groups of Y is dependent on the distribution of the groups in X (or vice versa).
  
## 3.2. Descriptive and Inferential Statistics. 
  
Assignment: *Provide univariate descriptive statistics and appropriate plots for both variables. Provide a scatterplot of X and Y.  Transform both variables simultaneously using Box-Cox transformations. Using the transformed variables, run a correlation analysis and interpret.*  *Test the hypothesis that the correlation between these variables is 0 and provide a 99% confidence interval. Discuss the meaning of your analysis.*

**Univariate descriptive statistics for X (`GrLivArea`)**
  
First we introduce functions to provide summary statistics, histogram and Q-Q plot of a continuous variable
```{r}

desc_stat = function(tbl, var_name) {

  df = as.data.frame(tbl[[var_name]])
  names(df)=var_name
  print (summary(df[,var_name]))
  meanval = mean(df[,var_name])
  std = sd(df[,var_name])
  
  print (paste("Standard deviation:",std),quote = F)
  ggplot(df, aes(df[,var_name])) + 
    geom_histogram(bins = 30,inherit.aes = T) + 
    geom_vline(aes(xintercept=meanval),color="blue") +
    geom_vline(aes(xintercept=meanval-2*std),color="blue",linetype=2) +
    geom_vline(aes(xintercept=meanval+2*std),color="blue",linetype=2) +
    ggtitle(paste("Histogram, mean +/- 2SD of the variable",var_name))+
    labs(x = paste("Value of",var_name))
  
}

single_qqplot = function(tbl, var_name) {
  
  qqnorm(tbl[[var_name]])
  qqline(tbl[[var_name]])
}

```
  

```{r}
dataset = dplyr::select(dataset, X,Y)
desc_stat(dataset, 'X')

single_qqplot(dataset, 'X')

```

We can see from the summary statistics and the histogram that the variable X is only positive, unimodal, and distributed around the mean of 1515 with a right skew and a few extreme outliers above 4000. Most of the values lie within 2 standard deviations of the mean (range: [`r c(round(mean(dataset$X)-2*sd(dataset$X)),round(mean(dataset$X)+2*sd(dataset$X)))`]). The Q-Q plot shows a significant deviation from the normal distribution. 

Interpretation: on average, the above-ground living area of a house in the dataset is around 1516 sq.feet, with a few exceptions between 364 and 465 sq.feet or above 2567 sq.feet.

**Univariate descriptive statistics for Y (`SalePrice`)**

Repeating the approach for Y we get:

```{r}
desc_stat(dataset, 'Y')
single_qqplot(dataset, 'Y')
```
  
From the summary statistics and the histogram, we can see a very wide range of values between the minimum and maximum price. The variable also takes only positive values (as expected). The distribution is unimodal and centered at the mean of 180900, with a stronger right skew. Most of the values lie within 2 standard deviations of the mean (range: [`r c(round((mean(dataset$Y)-2*sd(dataset$Y))/1000),round((mean(dataset$Y)+2*sd(dataset$Y))/1000))` ]thousand dollars). The Q-Q plot shows a significant deviation from the normal distribution in the right tail.
  
Interpretation: on average, the selling price of a house in the dataset is around 181k USD, with 50% of the values between 130k and 214k.
  
**X-Y Scatterplot and Box-Cox transformation**
  
Now we'll inspect the relationship between X and Y.

Here is the scatterplot:

```{r}
qplot(data =dataset, x = X, y=Y,main = "Scatterplot of X and Y")
```
  
We can see a mostly linear relationship with a couple of outliers with a very high living area (X) and unusually low sale price (Y).

We will now apply Box-Cox transformations to each of the variables using the optimal lambda parameter to make each of the transformed distributions closer to a normal distribution.

First we identify the optimum $\lambda$ parameters for the Box-Cox transformation^[4][4],[5][5]^ that maximize the correlation between the transformed values of the variable and the normal distribution quantiles. Then we apply the Box-Cox transformation on each of the variables using their optimal lambda values. The functions from the library `forecast` can be used for this:

```{r}
library(forecast)

lambda_x = BoxCox.lambda(dataset$X,method = 'loglik',-2,2)
lambda_y = BoxCox.lambda(dataset$Y,method = 'loglik',-2,2)

dataset$tr_X = BoxCox(dataset$X,lambda_x)
dataset$tr_Y = BoxCox(dataset$Y,lambda_y)

```

Now we inspect the transformed variables of X and Y.

```{r}
qplot(x = tr_X, data = dataset,main = "Histogram of the Box-Cox transformed X variable")
qplot(x = tr_Y, data = dataset,main = "Histogram of the Box-Cox transformed Y variable")

```
  
Both distributions now show significantly less skew.
  
An updated scatterplot for the transformed variables still shows a positive linear relationship with a few outliers:
```{r}
qplot(data =dataset, x = tr_X, y=tr_Y,main = "Scatterplot of transformed X and Y")

```

  
**Correlation Analysis**
  
We can now proceed to the correlation analysis on the transformed variables.
  
Running a correlation test on the transformed X and Y variables we get the following results:

```{r}
cor.test(dataset$tr_X,dataset$tr_Y,conf.level = 0.99)
```

Interpretation: due to the very low p-value (p.v.<<0.01) of the test statistic we reject H~0~ that there is no correlation between `tr_X` and `tr_Y`(the transformed X and Y variables) in favor of the H~A~ that there is a correlation between the variables at the 99% confidence level.

The 99% confidence interval for the correlation between `tr_X` and `tr_Y` is: [0.696; 0.759].
So there is a fairly strong positive linear relationship between the variables (as shown in the scatterplot above).   
In other words, an the higher the (transformed) size of the living area of a house, the higher is its (transformed) sales price.

## 3.3. Linear Algebra and Correlation
  
Assignment: *Invert your correlation matrix. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.*
  
First we construct the correlation matrix:

```{r}
t_dataset = dplyr::select(dataset, tr_X, tr_Y)
cor_mat = cor(t_dataset)
cor_mat
```
  
Now inverting the correlation matrix we get the precision matrix:
```{r}
inv_cor_mat = solve(cor_mat)
inv_cor_mat
```
  
Multiplying the correlation matrix by the precision matrix we by definition get the identity matrix, as one matrix is the inverse of the other:
```{r}
result1 = cor_mat %*% inv_cor_mat
round(result1)
```
  
Multiplying the precision matrix by the correlation matrix we also get the identity matrix.

```{r}
result2 = inv_cor_mat  %*% cor_mat
round(result2)
```

## 3.4. Calculus-Based Probability & Statistics  
  
Assignment: *Many times, it makes sense to fit a closed form distribution to data.  For your non-transformed independent variable, location shift it so that the minimum value is above zero. Then load the MASS package and run fitdistr to fit a density function of your choice. Find the optimal value of the parameters for this distribution, and then take 1000 samples from this distribution.  Plot a histogram and compare it with a histogram of your non-transformed original variable.*   

First, make sure that the minimum value is above zero in the non-transformed variable X (although this should be the case, as the living area of the house is always >0):

```{r}
max(dataset$X)>0
```

Now we can proceed to finding a fitting density function to the sample distribution so that we can generate multiple samples using the PDF. As the values in our sample are only positive, it might be reasonable to fit a Gamma distribution:

```{r}

pdf_fit = fitdistr(dataset$X,densfun = 'gamma')

print (pdf_fit)
```

Using the optimum parameters identified by the `fitdistr` function^[6][6]^, we get the new PDF that should generate observations that are similar to the original sample distribution and take 1000 samples from this distribution.
Plotting the two histograms on top of each other (original data vs. the generated data) we should see a close match:

```{r}
# Generate the same amount of samples using the estimated PDF as in the original dataset
new_X = rgamma(n = 1000,shape = pdf_fit$estimate[1],rate = pdf_fit$estimate[2])

# Plot both the original sampling distribution and the newly generated X values (sample 1000 from the original dataset)

dataset_1000 = dataset %>% sample_n(1000)

ggplot(data=dataset_1000) + 
  geom_histogram(aes(X, fill = "Original")) + 
  geom_histogram(aes(new_X, fill="New PDF"),alpha=0.5)+
  ggtitle("Sampling distribution of X: original dataset vs. estimated PDF")+
  scale_fill_manual("Source of the samples", values = c("red", "blue"))

```
  
We observe a fairly close match in the distribution of a 1000 samples from the original data vs. the estimated probability density function from the Gamma family.

## 3.5 Modeling

Assignment: *Build some type of regression model and submit your model to the competition board. Provide your complete model summary and results with analysis.*  

As the original dataset includes a large number of predictor variables (80), overfitting is a concern when building the model with the full set of variables. On the other hand, simple best model selection approaches would be inefficient given the large number of combinations of the possible predictors.  
Therefore we'll apply a regularized regression approach that automatically takes care of the variable selection by setting predictor coefficients to zero if this helps to reduce the overall error (elastic net regression from the `caret` library)^[7][7][8][8]^.   
The optimal regression hyperparameters $\lambda$ (penalty parameter) and $\alpha$ (the elasticnet mixing parameter) will be identified using k-fold cross-validation, then the model will be trained using the defined best lambda and alpha values.  

### 3.5.1 Data preprocessing

First, we need to preprocess the data: remove near-zero variance predictors and generate numeric dummy variables for factor variables^[8][8]^.
  
Reading the dataset and splitting into training and test:
```{r read data, cache=T}
# Read both datasets into one dataframe to get the full set of factor levels.
# Otherwise GLM methods relying on matrices with dummy variables for each factor level will crash

train = read_csv("https://raw.githubusercontent.com/datafeelings/data_605_final_project/master/data/train.csv")
test = read_csv("https://raw.githubusercontent.com/datafeelings/data_605_final_project/master/data/test.csv")

train = data.frame(train)
test = data.frame(test)

test$SalePrice = 0 # compensate for the missing response

# Add definition
train$subset = "train"
test$subset = "test"

full = rbind(train,test)

# Convert all characters to factors
for(i in 1:ncol(full)) {
  if(is.character(full[,i])) {
    full[,i] = as.factor(full[,i])
    }
}

# Now we can split the datasets back and proceed separate processing

inp_df = full[full$subset=="train",1:(ncol(full)-1)]
test = full[full$subset=="test",1:(ncol(full)-1)]
```

We preprocess the dataset removing the variables with near-zero variance that can lead to high-leverage observations and then convert the training dataframe of into a design matrix with dummy variables for each factor level.

```{r caret train preproc}
library(caret)

# Upon inspection of the codebook, we make 2 changes:
#   the MSSubClass should be dropped as it confuses the prediction
#   the Id column should be excluded

inp_df = inp_df[,3:ncol(inp_df)]  


# We should exclude the variables having very low variance that might lead to observations with an extreme leverage in individual folds

nzv <- nearZeroVar(inp_df, saveMetrics= TRUE)

nonzv_params = nzv[nzv$nzv==FALSE,]
nonzv_params = row.names(nonzv_params)

inp_no_nzv = inp_df[,nonzv_params]


# Generate a feature matrix with dummy variables and 
# replace NA values in the feature matrix with zeros
dummies = dummyVars(SalePrice ~ ., data = inp_no_nzv)
input_vars = predict(dummies,inp_no_nzv)
input_vars[is.na(input_vars)] = 0


```

### 3.5.2 Data Modeling & Diagnostics

Now we fit an elastic net regression using 10-fold cross-validation repeated 5 times.
During the repeated CV-fitting process, the `caret::train` function^[8][8]^ will identify a set of optimum regression hyperparameters (alpha and lambda).

```{r caret lasso-fit, cache=TRUE}
set.seed(123)

fitControl <- trainControl(## 10-fold CV
                           method = "adaptive_cv",
                           number = 10,
                           ## repeated ten times
                           repeats = 5)


lasso_fit = train(x=input_vars,y=inp_df$SalePrice, 
                  method = "glmnet",
                  trControl = fitControl)
lasso_fit

```
  
We can see that the best model explains approximately 84% of the variance in the response and has an RMSE of approximately 32 thousand dollars.

Now we can use the final model to compare its output with the in-sample data first:

```{r}
# Final model
lasso_model = lasso_fit$finalModel

fitted = predict(lasso_fit)

qplot(x=fitted, y = inp_df$SalePrice,xlab = "Fitted data", ylab = "True Response",
      main = "Predicted Sale Price vs. True Response")
```
  
The plot looks fairly good with an exception of a few outliers not captured by the model.
We also see an impact of a few extreme outliers on the residual plots^[9][9]^ below:

```{r fig.height=6}
library(plotmo)

plotres(lasso_model,which = 3:4)

```
  
While model residuals look fine for the most part, heteroscedasticity is visible, and the normal quantile plot shows the same outliers corrupting the residual distribtion.  

In the future iterations, a clear avenue of model improvement would be outlier analysis in the training data and feature engineering to deal with such cases appropriately (e.g. the abovementioned analysis of the `GrLivArea` variable and its Box-Cox transformation already provides some possibilities).  

### 3.5.3 Model interpretation 

Inspecting the variable importance of the model^[8][8]^ we identify the top factors influencing the price:

```{r}
var_imp = varImp(lasso_model, scale = FALSE)
var_imp = tbl_df(var_imp)
var_imp$variable = row.names(var_imp)
var_imp = var_imp %>% arrange(-Overall)
var_imp[1:10,]
```

We can see that among the top-10 parameters by the strength of their influence are the excellent levels of pool, kitchen, basement, and exterior material quality, as well as certain neighborhoods.  
The model is easily interpretable using common sense.  
   
### 3.5.4 Kaggle Submission
  
Now we can apply the model to generate predictions from the test data in order to submit them to the Kaggle competition.

```{r caret test set}

# Apply the same transformations as on the training set

test_df = test[,3:ncol(test)]
test_df = test_df[,nonzv_params] # retain only the selected non-NZV columns

# Generate a feature matrix with dummy variables and 
# replace NA values in the feature matrix with zeros
dummies_test = dummyVars(SalePrice ~ ., data = test_df)
input_vars_test = predict(dummies_test,test_df)
input_vars_test[is.na(input_vars_test)] = 0

# Predict for the test data

test_pred = predict(lasso_model,s = lasso_model$lambdaOpt, newx = input_vars_test,type="link")
```

Save the predicted dataset in the format required by Kaggle:
```{r}
test_pred = data.frame(test_pred)
names(test_pred) = "SalePrice"
test_pred$Id = test$Id
test_pred = test_pred[,c("Id","SalePrice")]

write.csv(test_pred, "test_pred.csv", row.names = F)
```

The Kaggle score of this submission is **0.13567**




# Reference

[1]: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data  "Kaggle Dataset Description"
[2]: http://ggplot2.tidyverse.org/reference/geom_histogram.html "ggplot2 Reference" 
[3]: http://www.r-tutor.com/elementary-statistics/goodness-fit/chi-squared-test-independence "R-Tutor: Chi Squared Test" 
[4]: https://www.rdocumentation.org/packages/car/versions/2.1-4/topics/boxCox "R Documentation: car package"  
[5]: http://www.itl.nist.gov/div898/handbook/eda/section3/eda336.htm "Engineering Statistics Handbook: Box-Cox Normality Plot"    
[6]: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html "R Documentation: Maximum-likelihood Fitting of Univariate Distributions"
[7]: http://www-bcf.usc.edu/~gareth/ISL/ "Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani: An Introduction to Statistical Learning with Applications in R"
[8]: http://topepo.github.io/caret/ "Max Kuhn: The caret Package"
[9]: http://www.milbo.org/doc/plotres-notes.pdf "Stephen Milborrow: Plotting model residuals with plotres"

1.[Kaggle Dataset Description](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)    
2. [ggplot2 Reference](http://ggplot2.tidyverse.org/reference/geom_histogram.html)  
3. [R-Tutor: Chi Squared Test](http://www.r-tutor.com/elementary-statistics/goodness-fit/chi-squared-test-independence)  
4. [R Documentation: Box-Cox Transformations For Linear Models](https://www.rdocumentation.org/packages/car/versions/2.1-4/topics/boxCox)  
5. [Engineering Statistics Handbook: Box-Cox Normality Plot](http://www.itl.nist.gov/div898/handbook/eda/section3/eda336.htm)     
6. [R Documentation: Maximum-likelihood Fitting of Univariate Distributions](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html)  
7. [Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani: An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/)  
8. [Max Kuhn: The caret Package](http://topepo.github.io/caret/)  
9. [Stephen Milborrow: Plotting model residuals with plotres](http://www.milbo.org/doc/plotres-notes.pdf)  